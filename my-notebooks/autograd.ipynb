{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd\n",
    "- Autograd is a automatic differentiation tool used to calculate derivatives (for training Neural Networks) in pytorch.\n",
    "- Autograd is a core component of pytorch that provides automatic differentiation for tensor operations. It enables gradient computation, which is essential for training machine learning modes using optimization algorithms like gradient descent.\n",
    "\n",
    "- Autograd solves the problem of differentiating nested functions using chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch;\n",
    "import math;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx at x=55: 110\n"
     ]
    }
   ],
   "source": [
    "# derivative function of y=x**2\n",
    "def dy_dx(x):\n",
    "    return 2*x;\n",
    "x = 55;\n",
    "print(f\"dy/dx at x={x}: {dy_dx(x)}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Process of a simple Neural Network:\n",
    "1. Forward Pass - Compute the output of the network given an input.\n",
    "2. Calculate Loss - Calculate the loss function to quantify the error.\n",
    "3. Backward Pass - Compute gradients (partial derivative) of the loss with respect to multiple parameters (weight, bias).\n",
    "4. Update gradients - Adjust the parameters using an optimization algorithm (ex: gradient descent).\n",
    "\n",
    "- Neural networks behave like a nested function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([4.], requires_grad=True) y: tensor([16.], grad_fn=<PowBackward0>)\n",
      "tensor([8.])\n",
      "x: tensor([10.])\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "# Autograd example 1\n",
    "# Leaf tensor/node - Input x, Root tensor/node - Output y=f(x)\n",
    "\n",
    "x = torch.tensor([4], dtype=torch.float32, requires_grad=True);\n",
    "# y=f(x)=x**2\n",
    "y = x**2;\n",
    "print(\"x:\", x, \"y:\", y);\n",
    "\n",
    "# Start calculating dy/dx\n",
    "y.backward(retain_graph=True);\n",
    "# Get dy/dx at x=4\n",
    "print(x.grad);\n",
    "\n",
    "# sets requires_grad to False for x.\n",
    "x.requires_grad_(False);\n",
    "x[0] = 10;\n",
    "print(\"x:\", x);\n",
    "# y.backward();  # not allowed\n",
    "print(x.grad);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([4.], requires_grad=True) y: tensor([16.], grad_fn=<PowBackward0>) z: tensor([-0.2879], grad_fn=<SinBackward0>)\n",
      "tensor([-7.6613])\n"
     ]
    }
   ],
   "source": [
    "# Autograd example 2\n",
    "# Leaf tensor/node - Input x, Root tensor/node - Output z=f(y), Intermediate tensor/node - y=f(x)\n",
    "# In a computational graph, gradients are not automatically computed for intermediated nodes. (pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "x = torch.tensor([4], requires_grad=True, dtype=torch.float32);\n",
    "# y=f(x)=x**2\n",
    "y = x**2;\n",
    "# z=f(y)=sin(y)\n",
    "z = torch.sin(y);\n",
    "\n",
    "print(\"x:\", x, \"y:\", y, \"z:\", z);\n",
    "\n",
    "# Start calculating dz/dx\n",
    "z.backward();\n",
    "# Get dz/dx at x=4\n",
    "print(x.grad);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd example 3\n",
    "##### Data set of a student's cgpa and whether he is placed (1) or not (0)\n",
    "| cgpa (x) | is_placed (y) |\n",
    "| ----- | ----- |\n",
    "| 6.7 | 0 |\n",
    "| 7.6 | 1|\n",
    "| 8.5 | 1 |\n",
    "\n",
    "1. Linear Transformation (z):\n",
    "<center> $z = w.x + b$ where `w` = weight & `b` = bias </center>\n",
    "\n",
    "\n",
    "2. Activation (Sigmoid function) ($\\hat{y}$ OR $y_{pred}$):\n",
    "<center> $\\hat{y} = y_{pred} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ </center>\n",
    "\n",
    "3. Loss Function (Binary Cross-Entropy Loss) (L):\n",
    "<center> $L = -[y.ln(y_{pred}) + (1 - y)ln(1 - y_{pred})]$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "x = torch.tensor(6.7, dtype=torch.float32);\n",
    "# expected output\n",
    "y = torch.tensor(0, dtype=torch.float32);\n",
    "\n",
    "# assumed weight, bias\n",
    "w = torch.tensor(1, dtype=torch.float32, requires_grad=True);\n",
    "b = torch.tensor(0, dtype=torch.float32, requires_grad=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7000, grad_fn=<AddBackward0>)\n",
      "tensor(0.9988, grad_fn=<SigmoidBackward0>)\n",
      "0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "z = w*x + b;\n",
    "# predicted output\n",
    "y_pred = torch.sigmoid(z);\n",
    "\n",
    "print(z);\n",
    "print(y_pred);\n",
    "print(f\"{y_pred}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7012, grad_fn=<NegBackward0>)\n",
      "Loss: 6.701176166534424\n"
     ]
    }
   ],
   "source": [
    "# binary cross entropy loss\n",
    "def binary_cross_entropy_loss(pred, target):\n",
    "    int_small = 1e-8;\n",
    "    prediction = torch.clamp(pred, max=(1 - int_small), min=int_small);\n",
    "    return -(target * torch.log(prediction) + ((1 - target) * torch.log(1 - prediction)));\n",
    "\n",
    "loss = binary_cross_entropy_loss(y_pred, y);\n",
    "print(loss);\n",
    "print(f\"Loss: {loss}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918) 6.6917619705200195\n",
      "tensor(0.9988) 0.9987704753875732\n"
     ]
    }
   ],
   "source": [
    "loss.backward();\n",
    "\n",
    "print(w.grad, f\"{w.grad}\");\n",
    "print(b.grad, f\"{b.grad}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
